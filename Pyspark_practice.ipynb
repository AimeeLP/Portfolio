{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to import pyspark to complete our analysis; pyspark is a large library with SQL and machine learning\n",
    "#It's first necessary to find where it is\n",
    "#Once found, import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "\n",
    "#this line is not necessary as the code returns the same output before and after importing pyspark\n",
    "#findspark.find() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line is not necessary as we have already imported pyspark\n",
    "#import pyspark\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession #this line imports an additonal libarary necessary for us to use SQL\n",
    "\n",
    "#It's necessary to define the connection requirements between spark and python\n",
    "#The first step is stating where the connection should be made (i.e. local)\n",
    "#Then we limit the memory to be used, as the computer may not have the capacity to handle all of the data\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .config(\"spark.executor.memory\", \"2gb\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "#It's then necessary to set the connection, otherwise known as the Context\n",
    "#We do this for spark, and for sql\n",
    "sc = spark.sparkContext \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. \n",
    "#It is an immutable distributed collection of objects. \n",
    "#Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster.\n",
    "#Formally, an RDD is a read-only, partitioned collection of records.\n",
    "#Spark makes use of the concept of RDD to achieve faster and efficient MapReduce operations.\n",
    "\n",
    "#We need to import the taxi data as an RDD in order to analyse using spark\n",
    "data = sc.textFile(\"Taxi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we confirm the \"type\" of the data that has been imported\n",
    "#This line has been moved to appear after original import of data rather than after checking the data values\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount',\n",
       " '',\n",
       " '1,2018-12-01 00:28:22,2018-12-01 00:44:07,2,2.50,1,N,148,234,1,12,0.5,0.5,3.95,0,0.3,17.25',\n",
       " '1,2018-12-01 00:52:29,2018-12-01 01:11:37,3,2.30,1,N,170,144,1,13,0.5,0.5,2.85,0,0.3,17.15',\n",
       " '2,2018-12-01 00:12:52,2018-12-01 00:36:23,1,.00,1,N,113,193,2,2.5,0.5,0.5,0,0,0.3,3.8',\n",
       " '1,2018-12-01 00:35:08,2018-12-01 00:43:11,1,3.90,1,N,95,92,1,12.5,0.5,0.5,2.75,0,0.3,16.55',\n",
       " '1,2018-12-01 00:21:54,2018-12-01 01:15:13,1,12.80,1,N,163,228,1,45,0.5,0.5,9.25,0,0.3,55.55',\n",
       " '1,2018-12-01 00:00:38,2018-12-01 00:29:26,1,18.80,1,N,132,97,1,50.5,0.5,0.5,10.35,0,0.3,62.15',\n",
       " '1,2018-12-01 00:59:39,2018-12-01 01:09:07,1,1.00,1,N,246,164,1,7.5,0.5,0.5,0.44,0,0.3,9.24',\n",
       " '1,2018-12-01 00:19:19,2018-12-01 00:22:19,1,.30,1,N,161,163,4,4,0.5,0.5,0,0,0.3,5.3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the first n elements of the RDD\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the columns names\n",
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is irrelevant as we have already explored the column names using the data.first()\n",
    "#data.top(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text files are difficult to analyse, so using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import a subset of data (from a new csv file) in pyspark using the SQL Context previously defined\n",
    "#This creates a new dataframe that allows us to use SQL commands, unlike the textfile we originally imported to view the data\n",
    "data2  = sqlContext.read.csv(\"taxi_dec.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code confirms the 'type' of the data post-import\n",
    "type(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show column names and data type of each\n",
    "data2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count: \n",
      "8173231\n",
      "Column Count: \n",
      "17\n"
     ]
    }
   ],
   "source": [
    "#Review the shape of the database before removal of null values; row count and column count\n",
    "print(\"Row Count: \")\n",
    "print(data2.count())\n",
    "print(\"Column Count: \")\n",
    "print(len(data2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string, RatecodeID: string, store_and_fwd_flag: string, PULocationID: string, DOLocationID: string, payment_type: string, fare_amount: string, extra: string, mta_tax: string, tip_amount: string, tolls_amount: string, improvement_surcharge: string, total_amount: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove all null values within the database\n",
    "data2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count: \n",
      "8173231\n",
      "Column Count: \n",
      "17\n",
      "Column Names: \n",
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n"
     ]
    }
   ],
   "source": [
    "#Review the shape of the database following removal of null values; row count, column count and column names\n",
    "#Cell originally had three lines of code, but so only the last output was shown\n",
    "#Code adjusted to show each output with a header\n",
    "print(\"Row Count: \")\n",
    "print(data2.count())\n",
    "print(\"Column Count: \")\n",
    "print(len(data2.columns))\n",
    "print(\"Column Names: \")\n",
    "print(data2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   passenger_count|\n",
      "+-------+------------------+\n",
      "|  count|           8173231|\n",
      "|   mean|1.5964102813195908|\n",
      "| stddev| 1.233920232393633|\n",
      "|    min|                 0|\n",
      "|    max|                 9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This code has been moved down to appear after checking the database shape\n",
    "#Here we're looking at some stats around the data held; specifically around the number of passengers\n",
    "data2.select(\"passenger_count\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|   passenger_count|     trip_distance|      total_amount|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|           8173231|           8173231|           8173231|\n",
      "|   mean|1.5964102813195908|2.8926264215460553|16.524363505886303|\n",
      "| stddev| 1.233920232393633| 3.764338945224816|120.42046343163905|\n",
      "|    min|                 0|               .00|             -0.31|\n",
      "|    max|                 9|             99.07|             99.98|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View several stats from the database side by side; compare high level view of passengers, distance and cost of trips\n",
    "data2.select('passenger_count', 'trip_distance', 'total_amount').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       0|                   0|                    0|              0|            0|         0|                 0|           0|           0|           0|          0|    0|      0|         0|           0|                    0|           0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import additional SQL functions to aid analysis\n",
    "#Use the new functions to create a table based on the set criteria from the database\n",
    "#This is used to check whether all null values were in fact removed; anticipate all zeros\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "data2.select([count(when(isnan(c), c)).alias(c) for c in data2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is irrelvant as we've already dropped null values and then counted the rows\n",
    "#The code is more efficient so could be used above instead of the existing code\n",
    "#data2.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter by passenger code, then total amount, then total amount and distance\n",
    "data = data2.filter((data2.passenger_count <= 4) & (data2.passenger_count > 0))\n",
    "data = data.filter(data.total_amount >= 2.5) #the law states minimum cost of taxi journey is $2.50\n",
    "data = data.filter((data.total_amount < 200) & (data.trip_distance < 100)) # total amount is never more than 99, remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(data.trip_distance > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|   passenger_count|      total_amount|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           5495209|           5495209|\n",
      "|   mean|  1.34628091488422| 19.36391156930136|\n",
      "| stddev|0.6915259106620711|14.465422201705064|\n",
      "|    min|                 1|                10|\n",
      "|    max|                 4|             99.98|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('passenger_count', 'total_amount').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------------+\n",
      "|summary|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+-------+--------------------+---------------------+\n",
      "|  count|             5495209|              5495209|\n",
      "|   mean|                null|                 null|\n",
      "| stddev|                null|                 null|\n",
      "|    min| 2003-12-23 05:01:51|  2003-12-23 05:09:00|\n",
      "|    max| 2020-12-10 20:34:26|  2020-12-10 20:54:46|\n",
      "+-------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('tpep_pickup_datetime', 'tpep_dropoff_datetime').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickups = data.select(col('PULocationID').alias('LocationID'))\\\n",
    "                        .groupBy('LocationID')\\\n",
    "                        .count()\\\n",
    "                        .sort('LocationID')\\ #irrelevant unless viewing data\n",
    "                        .select(col('count').alias('pickups'), col('LocationID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs = data.select(col('DOLocationID')\\\n",
    "                        .alias('LocationID'))\\\n",
    "                        .groupBy('LocationID')\\\n",
    "                        .count()\\\n",
    "                        .sort('LocationID')\\ #irrelevant unless viewing data\n",
    "                        .select(col('count').alias('dropoffs'), col('LocationID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_drops = pickups.join(dropoffs, on= ['LocationID']) #inner join as nothing defined, so default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n",
      "|LocationID|pickups|dropoffs|\n",
      "+----------+-------+--------+\n",
      "|       125|  30437|   26820|\n",
      "|       124|    210|     858|\n",
      "|        51|    544|    1388|\n",
      "|         7|   8222|   28315|\n",
      "|       169|    277|    1530|\n",
      "|       205|    575|    1398|\n",
      "|        15|     88|     651|\n",
      "|       232|   9827|   28553|\n",
      "|       234| 156298|  125827|\n",
      "|        54|    202|    1722|\n",
      "|       155|    470|    1204|\n",
      "|       132| 165174|   54698|\n",
      "|       154|     27|      70|\n",
      "|       200|    227|    2382|\n",
      "|       101|    159|     544|\n",
      "|        11|    102|     621|\n",
      "|       138| 188565|   78204|\n",
      "|        29|    140|     750|\n",
      "|        69|    662|    3426|\n",
      "|       112|   2182|   15268|\n",
      "+----------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "picks_drops.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n",
      "|LocationID|pickups|dropoffs|\n",
      "+----------+-------+--------+\n",
      "|       125|  30437|   26820|\n",
      "|       124|    210|     858|\n",
      "|        51|    544|    1388|\n",
      "|         7|   8222|   28315|\n",
      "|       169|    277|    1530|\n",
      "|       205|    575|    1398|\n",
      "|        15|     88|     651|\n",
      "|       232|   9827|   28553|\n",
      "|       234| 156298|  125827|\n",
      "|        54|    202|    1722|\n",
      "|       155|    470|    1204|\n",
      "|       132| 165174|   54698|\n",
      "|       154|     27|      70|\n",
      "|       200|    227|    2382|\n",
      "|       101|    159|     544|\n",
      "|        11|    102|     621|\n",
      "|       138| 188565|   78204|\n",
      "|        29|    140|     750|\n",
      "|        69|    662|    3426|\n",
      "|       112|   2182|   15268|\n",
      "+----------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#duplicate code, removed\n",
    "#picks_drops.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toPandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5616010e0fc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpicks_drops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpicks_drops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mT:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toPandas'"
     ]
    }
   ],
   "source": [
    "picks_drops = picks_drops.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_drops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_drops['total'] = picks_drops['pickups'] + picks_drops['dropoffs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_map = geopandas.read_file('taxi_zones/taxi_zones.shp')\n",
    "NYC_map.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_map.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_drops['LocationID'] = picks_drops['LocationID'].astype(int)\n",
    "picks_drops.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC = NYC_map.merge(picks_drops, on = 'LocationID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not correspond to any plot so irrelevant, removed\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line is unnecessary, removed\n",
    "#fig, ax = plt.subplots(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC.plot(column = 'total', cmap='OrRd', scheme='quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pickups = NYC[['pickups', 'zone', 'borough']].sort_values('pickups', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pickups.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
